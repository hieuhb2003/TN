{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Đọc file Parquet\n",
    "df = pd.read_parquet('')\n",
    "df = df.head(100000)\n",
    "print(f\"Số dòng sau khi giới hạn: {len(df)}\")\n",
    "\n",
    "# Chia dữ liệu thành các phần để xử lý song song\n",
    "def split_dataframe(df, chunks):\n",
    "    return np.array_split(df, chunks)\n",
    "\n",
    "# Hàm xử lý trên từng phần DataFrame với tqdm\n",
    "def process_chunk(df_chunk, language='vi'):\n",
    "    query_chunks = {}\n",
    "    query_col = 'query' if language == 'vi' else 'query_en'\n",
    "    \n",
    "    for index, row in tqdm(df_chunk.iterrows(), total=len(df_chunk), \n",
    "                          desc=f\"Xử lý {language}\", leave=False):\n",
    "        query = row[query_col]\n",
    "        \n",
    "        if query not in query_chunks:\n",
    "            query_chunks[query] = []\n",
    "        \n",
    "        chunk = {\n",
    "            'pos': row['pos'],\n",
    "            'pos_en': row['pos_en']\n",
    "        }\n",
    "        query_chunks[query].append(chunk)\n",
    "    \n",
    "    return query_chunks\n",
    "\n",
    "# Hàm gộp kết quả từ các phần xử lý\n",
    "def merge_dictionaries(dict_list):\n",
    "    result = {}\n",
    "    print(\"Đang gộp kết quả từ các luồng...\")\n",
    "    for d in tqdm(dict_list):\n",
    "        for key, value in d.items():\n",
    "            if key in result:\n",
    "                result[key].extend(value)\n",
    "            else:\n",
    "                result[key] = value\n",
    "    return result\n",
    "\n",
    "# Số lượng CPU cores để tận dụng\n",
    "num_processes = min(4, mp.cpu_count())\n",
    "print(f\"Sử dụng {num_processes} luồng xử lý\")\n",
    "\n",
    "\n",
    "# Chia DataFrame thành các phần\n",
    "print(\"Đang chia dữ liệu thành các phần...\")\n",
    "df_chunks = split_dataframe(df, num_processes)\n",
    "\n",
    "# Tạo pool processes\n",
    "pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "# Xử lý song song cho tiếng Việt\n",
    "print(\"Bắt đầu xử lý dữ liệu tiếng Việt...\")\n",
    "process_chunk_vi = partial(process_chunk, language='vi')\n",
    "results_vi = list(tqdm(pool.imap(process_chunk_vi, df_chunks), \n",
    "                      total=num_processes, desc=\"Tiến trình VI\"))\n",
    "query_chunks_vie = merge_dictionaries(results_vi)\n",
    "\n",
    "# Xử lý song song cho tiếng Anh\n",
    "print(\"Bắt đầu xử lý dữ liệu tiếng Anh...\")\n",
    "process_chunk_en = partial(process_chunk, language='en')\n",
    "results_en = list(tqdm(pool.imap(process_chunk_en, df_chunks), \n",
    "                      total=num_processes, desc=\"Tiến trình EN\"))\n",
    "query_chunks_en = merge_dictionaries(results_en)\n",
    "\n",
    "# Đóng pool\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "print(f\"Số lượng query tiếng Việt duy nhất: {len(query_chunks_vie)}\")\n",
    "print(f\"Số lượng query tiếng Anh duy nhất: {len(query_chunks_en)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_vie - pos_en\n",
    "import json\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hàm xử lý từng nhóm query\n",
    "def process_query_chunk(queries_chunk):\n",
    "    result = {}\n",
    "    for query, dict_chunks in queries_chunk:\n",
    "        pos_set = set()\n",
    "        pos_en_set = set()\n",
    "        \n",
    "        for item in dict_chunks:\n",
    "            for chunk in item['pos']:\n",
    "                pos_set.add(chunk)\n",
    "            for chunk in item['pos_en']:\n",
    "                pos_en_set.add(chunk)\n",
    "        \n",
    "        result[query] = list(pos_en_set)\n",
    "        \n",
    "    \n",
    "    return result\n",
    "\n",
    "# Hàm để chia list thành các phần bằng nhau\n",
    "def split_list(items, num_chunks):\n",
    "    \"\"\"Chia list thành các phần bằng nhau\"\"\"\n",
    "    avg = len(items) // num_chunks\n",
    "    remainder = len(items) % num_chunks\n",
    "    \n",
    "    result = []\n",
    "    start = 0\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Thêm một phần tử vào mỗi phần cho đến khi hết remainder\n",
    "        end = start + avg + (1 if i < remainder else 0)\n",
    "        result.append(items[start:end])\n",
    "        start = end\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Hàm chính để merge positions với đa luồng\n",
    "def merge_positions_parallel(data, output_file, num_processes=None):\n",
    "    # Xác định số lượng CPU sẽ sử dụng\n",
    "    if num_processes is None:\n",
    "        num_processes = mp.cpu_count()\n",
    "    \n",
    "    print(f\"Sử dụng {num_processes} luồng xử lý\")\n",
    "    \n",
    "    # Chuyển dict thành list các tuples\n",
    "    items = list(data.items())\n",
    "    \n",
    "    # Chia thành các phần bằng nhau\n",
    "    chunks = split_list(items, num_processes)\n",
    "    \n",
    "    # Tạo pool processes\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "    \n",
    "    # Thực hiện xử lý song song\n",
    "    print(\"Đang xử lý dữ liệu...\")\n",
    "    results = list(tqdm(pool.imap(process_query_chunk, chunks), total=len(chunks)))\n",
    "    \n",
    "    # Đóng pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # Gộp các kết quả\n",
    "    merged_data = {}\n",
    "    print(\"Đang gộp kết quả từ các luồng...\")\n",
    "    for result in tqdm(results):\n",
    "        merged_data.update(result)\n",
    "    \n",
    "    # Tạo thư mục đầu ra nếu chưa tồn tại\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Lưu kết quả vào file mới\n",
    "    print(f\"Đang lưu dữ liệu đã gộp vào: {output_file}\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"Hoàn thành!\")\n",
    "    \n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "# Đường dẫn đến file\n",
    "\n",
    "output_file = 'grouth_truth_query_vie_pos_en.json'\n",
    "\n",
    "data=query_chunks_vie\n",
    "\n",
    "# Gọi hàm xử lý đa luồng\n",
    "merge_positions_parallel(data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_en - pos_vie\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hàm xử lý từng nhóm query\n",
    "def process_query_chunk(queries_chunk):\n",
    "    result = {}\n",
    "    for query, dict_chunks in queries_chunk:\n",
    "        pos_set = set()\n",
    "        pos_en_set = set()\n",
    "        \n",
    "        for item in dict_chunks:\n",
    "            for chunk in item['pos']:\n",
    "                pos_set.add(chunk)\n",
    "            for chunk in item['pos_en']:\n",
    "                pos_en_set.add(chunk)\n",
    "        \n",
    "        result[query] = list(pos_set)\n",
    "  \n",
    "    return result\n",
    "\n",
    "# Hàm để chia list thành các phần bằng nhau\n",
    "def split_list(items, num_chunks):\n",
    "    \"\"\"Chia list thành các phần bằng nhau\"\"\"\n",
    "    avg = len(items) // num_chunks\n",
    "    remainder = len(items) % num_chunks\n",
    "    \n",
    "    result = []\n",
    "    start = 0\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        # Thêm một phần tử vào mỗi phần cho đến khi hết remainder\n",
    "        end = start + avg + (1 if i < remainder else 0)\n",
    "        result.append(items[start:end])\n",
    "        start = end\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Hàm chính để merge positions với đa luồng\n",
    "def merge_positions_parallel(data, output_file, num_processes=None):\n",
    "    # Xác định số lượng CPU sẽ sử dụng\n",
    "    if num_processes is None:\n",
    "        num_processes = mp.cpu_count()\n",
    "    \n",
    "    print(f\"Sử dụng {num_processes} luồng xử lý\")\n",
    "    \n",
    "    # Chuyển dict thành list các tuples\n",
    "    items = list(data.items())\n",
    "    \n",
    "    # Chia thành các phần bằng nhau\n",
    "    chunks = split_list(items, num_processes)\n",
    "    \n",
    "    # Tạo pool processes\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "    \n",
    "    # Thực hiện xử lý song song\n",
    "    print(\"Đang xử lý dữ liệu...\")\n",
    "    results = list(tqdm(pool.imap(process_query_chunk, chunks), total=len(chunks)))\n",
    "    \n",
    "    # Đóng pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # Gộp các kết quả\n",
    "    merged_data = {}\n",
    "    print(\"Đang gộp kết quả từ các luồng...\")\n",
    "    for result in tqdm(results):\n",
    "        merged_data.update(result)\n",
    "    \n",
    "    # Tạo thư mục đầu ra nếu chưa tồn tại\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Lưu kết quả vào file mới\n",
    "    print(f\"Đang lưu dữ liệu đã gộp vào: {output_file}\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"Hoàn thành!\")\n",
    "    \n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "# Đường dẫn đến file\n",
    "\n",
    "output_file = 'grouth_truth_query_en_pos_vie.json'\n",
    "\n",
    "data=query_chunks_en\n",
    "\n",
    "# Gọi hàm xử lý đa luồng\n",
    "merge_positions_parallel(data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kv_store_doc_status.json\",'r') as f:\n",
    "    smalL_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_context = []\n",
    "for  key, item in smalL_data.items():\n",
    "    # print(item)\n",
    "    if item['status'] == \"processed\":\n",
    "        small_context.append(item['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/example_benchmark/small_contexts.json\",'w', encoding='utf-8') as f:\n",
    "    json.dump(small_context,f,indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/grouth_truth_query_en_pos_vie.json\",'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "query_context = {}\n",
    "for query, chunks in data.items():\n",
    "    ls_chunks=[]\n",
    "    cout = 0\n",
    "    for chunk in chunks:\n",
    "        if chunk in small_context:\n",
    "            ls_chunks.append(chunk)\n",
    "    if len(ls_chunks) > 0:\n",
    "        queries.append(query)\n",
    "        query_context[query] = ls_chunks\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
