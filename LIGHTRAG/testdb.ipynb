{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "google/gemini-2.0-flash-exp:free\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "from lightrag.kg.shared_storage import initialize_pipeline_status\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm.hf import hf_embed\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from lightrag.llm.openai import openai_complete_if_cache\n",
    "from lightrag.utils import detect_language\n",
    "with open(\"C:\\\\Users\\\\mhieu\\\\Desktop\\\\TN\\\\LIGHTRAG\\\\api_keys.json\", 'r', encoding='utf-8') as f:\n",
    "    OPENROUTER_API_KEYS = json.load(f)\n",
    "\n",
    "class APIManager:\n",
    "    def __init__(self, api_keys: List[str]):\n",
    "        self.api_keys = api_keys\n",
    "        self.current_key_index = 0\n",
    "        self.failed_keys = set()\n",
    "        self.last_switch_time = {}  \n",
    "        \n",
    "    def get_current_api_key(self):\n",
    "        return self.api_keys[self.current_key_index]\n",
    "    \n",
    "    def switch_to_next_key(self):\n",
    "        \n",
    "        self.failed_keys.add(self.current_key_index)\n",
    "        self.last_switch_time[self.current_key_index] = time.time()\n",
    "        \n",
    "        available_keys = []\n",
    "        for idx in range(len(self.api_keys)):\n",
    "            if idx not in self.failed_keys:\n",
    "                available_keys.append(idx)\n",
    "            elif idx in self.last_switch_time:\n",
    "                # Nếu đã qua 10 phút kể từ lần cuối sử dụng key này\n",
    "                if time.time() - self.last_switch_time[idx] > 600:\n",
    "                    self.failed_keys.remove(idx)\n",
    "                    available_keys.append(idx)\n",
    "        \n",
    "        if not available_keys:\n",
    "            raise RuntimeError(\"Tất cả API keys đều đã thất bại. Vui lòng thử lại sau.\")\n",
    "        \n",
    "        self.current_key_index = random.choice(available_keys)\n",
    "        print(f\"Đã chuyển sang API key: {self.api_keys[self.current_key_index][:5]}...\")\n",
    "        return self.get_current_api_key()\n",
    "    \n",
    "    def reset_key(self, key_index):\n",
    "        if key_index in self.failed_keys:\n",
    "            self.failed_keys.remove(key_index)\n",
    "\n",
    "# Khởi tạo API Manager\n",
    "api_manager = APIManager(OPENROUTER_API_KEYS)\n",
    "\n",
    "WORKING_DIR = \"./test_duo\"\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    os.mkdir(WORKING_DIR)\n",
    "\n",
    "current_api_key = api_manager.get_current_api_key()\n",
    "    \n",
    "async def llm_model_func(\n",
    "    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n",
    ") -> str:\n",
    "    global current_api_key\n",
    "    max_retries = len(OPENROUTER_API_KEYS)\n",
    "    retry_count = 0\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            response = await openai_complete_if_cache(\n",
    "                \"google/gemini-2.0-flash-exp:free\",\n",
    "                prompt,\n",
    "                system_prompt=system_prompt,\n",
    "                history_messages=history_messages,\n",
    "                api_key=current_api_key, \n",
    "                base_url=os.getenv(\"LLM_BINDING_HOST\", \"https://openrouter.ai/api/v1\"),\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            key_index = OPENROUTER_API_KEYS.index(current_api_key)\n",
    "            api_manager.reset_key(key_index)\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi với API key: {str(e)}\")\n",
    "            retry_count += 1\n",
    "            \n",
    "            if retry_count < max_retries:\n",
    "                print(\"Đang chuyển sang API key tiếp theo...\")\n",
    "                current_api_key = api_manager.switch_to_next_key()\n",
    "                os.environ[\"LLM_BINDING_API_KEY\"] = current_api_key\n",
    "            else:\n",
    "                print(\"Tất cả API keys đều đã thất bại.\")\n",
    "                raise e\n",
    "    \n",
    "    raise RuntimeError(\"Tất cả API keys đều đã thất bại\")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "print(\"google/gemini-2.0-flash-exp:free\")\n",
    "\n",
    "os.environ[\"LLM_BINDING_API_KEY\"] = current_api_key\n",
    "async def initialize_rag():\n",
    "    rag = LightRAG(\n",
    "        working_dir=WORKING_DIR,\n",
    "        llm_model_func=llm_model_func,\n",
    "        embedding_func=EmbeddingFunc(\n",
    "            embedding_dim=1024,\n",
    "            max_token_size=5000,\n",
    "            func=lambda texts: hf_embed(\n",
    "                texts,\n",
    "                tokenizer=AutoTokenizer.from_pretrained(\n",
    "                    \"BAAI/bge-m3\"\n",
    "                ),\n",
    "                embed_model=AutoModel.from_pretrained(\n",
    "                    \"BAAI/bge-m3\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "        addon_params={\n",
    "            # \"insert_batch_size\": 20,\n",
    "            \"language\": \"Vietnamese\"\n",
    "        }\n",
    "    )\n",
    "    await rag.initialize_storages()\n",
    "    await initialize_pipeline_status()\n",
    "\n",
    "    return rag\n",
    "\n",
    "\n",
    "async def main():\n",
    "    rag = await initialize_rag()\n",
    "    entities_vdb = rag.entities_vdb\n",
    "    entity_id = \"ent-3ec1c563b6bb321152a391a90957a38b\"\n",
    "    # Use proper client._client.get method\n",
    "    result = entities_vdb._client.get([entity_id])\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Logger initialized for working directory: ./test_duo\n",
      "INFO:lightrag:Load KV llm_response_cache with 1 data\n",
      "INFO:lightrag:Load KV full_docs with 7 data\n",
      "INFO:lightrag:Load KV text_chunks with 7 data\n",
      "INFO:lightrag:Loaded graph from ./test_duo\\graph_chunk_entity_relation.graphml with 50 nodes, 48 edges\n",
      "INFO:nano-vectordb:Load (50, 1024) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './test_duo\\\\vdb_entities.json'} 50 data\n",
      "INFO:nano-vectordb:Load (48, 1024) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './test_duo\\\\vdb_relationships.json'} 48 data\n",
      "INFO:nano-vectordb:Load (7, 1024) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './test_duo\\\\vdb_chunks.json'} 7 data\n",
      "INFO:lightrag:Loaded document status storage with 8 records\n",
      "ERROR: Error: try to getnanmespace before it is initialized, pid=11516\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shared dictionaries not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     nest_asyncio.apply()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mhieu\\anaconda3\\envs\\rag\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mhieu\\anaconda3\\envs\\rag\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mhieu\\anaconda3\\envs\\rag\\Lib\\asyncio\\futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mhieu\\anaconda3\\envs\\rag\\Lib\\asyncio\\tasks.py:277\u001b[39m, in \u001b[36mTask.__step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    276\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    279\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     rag = \u001b[38;5;28;01mawait\u001b[39;00m initialize_rag()\n\u001b[32m    139\u001b[39m     entities_vdb = rag.entities_vdb\n\u001b[32m    140\u001b[39m     entity_id = \u001b[33m\"\u001b[39m\u001b[33ment-3ec1c563b6bb321152a391a90957a38b\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36minitialize_rag\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    110\u001b[39m rag = LightRAG(\n\u001b[32m    111\u001b[39m     working_dir=WORKING_DIR,\n\u001b[32m    112\u001b[39m     llm_model_func=llm_model_func,\n\u001b[32m   (...)\u001b[39m\u001b[32m    129\u001b[39m     }\n\u001b[32m    130\u001b[39m )\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m rag.initialize_storages()\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m initialize_pipeline_status()\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rag\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mhieu\\Desktop\\TN\\LIGHTRAG\\lightrag\\kg\\shared_storage.py:278\u001b[39m, in \u001b[36minitialize_pipeline_status\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitialize_pipeline_status\u001b[39m():\n\u001b[32m    274\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[33;03m    Initialize pipeline namespace with default values.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    This function is called during FASTAPI lifespan for each worker.\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     pipeline_namespace = \u001b[38;5;28;01mawait\u001b[39;00m get_namespace_data(\u001b[33m\"\u001b[39m\u001b[33mpipeline_status\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m get_internal_lock():\n\u001b[32m    281\u001b[39m         \u001b[38;5;66;03m# Check if already initialized by checking for required fields\u001b[39;00m\n\u001b[32m    282\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mbusy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pipeline_namespace:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mhieu\\Desktop\\TN\\LIGHTRAG\\lightrag\\kg\\shared_storage.py:428\u001b[39m, in \u001b[36mget_namespace_data\u001b[39m\u001b[34m(namespace)\u001b[39m\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _shared_dicts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    424\u001b[39m     direct_log(\n\u001b[32m    425\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError: try to getnanmespace before it is initialized, pid=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.getpid()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    426\u001b[39m         level=\u001b[33m\"\u001b[39m\u001b[33mERROR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    427\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mShared dictionaries not initialized\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m get_internal_lock():\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m namespace \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _shared_dicts:\n",
      "\u001b[31mValueError\u001b[39m: Shared dictionaries not initialized"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Logger initialized for working directory: ./test_duo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightrag:Load KV llm_response_cache with 1 data\n",
      "INFO:lightrag:Load KV full_docs with 13 data\n",
      "INFO:lightrag:Load KV text_chunks with 13 data\n",
      "INFO:lightrag:Loaded graph from ./test_duo\\graph_chunk_entity_relation.graphml with 75 nodes, 74 edges\n",
      "INFO:nano-vectordb:Load (88, 1024) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './test_duo\\\\vdb_entities.json'} 88 data\n",
      "INFO:nano-vectordb:Load (75, 1024) data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "google/gemini-2.0-flash-exp:free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './test_duo\\\\vdb_relationships.json'} 75 data\n",
      "INFO:nano-vectordb:Load (14, 1024) data\n",
      "INFO:nano-vectordb:Init {'embedding_dim': 1024, 'metric': 'cosine', 'storage_file': './test_duo\\\\vdb_chunks.json'} 14 data\n",
      "INFO:lightrag:Loaded document status storage with 14 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for entity ent-3ec1c563b6bb321152a391a90957a38b: [-0.00950563 -0.04885593 -0.01246002 ...  0.00220176 -0.02015256\n",
      "  0.0001128 ]\n",
      "[{'__id__': 'ent-3ec1c563b6bb321152a391a90957a38b', '__created_at__': 1743247001.27852, 'entity_name': '\"THÔNG TƯ\"'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "\n",
    "from lightrag import LightRAG, QueryParam\n",
    "from lightrag.llm.hf import hf_embed\n",
    "from lightrag.utils import EmbeddingFunc\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from lightrag.llm.openai import openai_complete_if_cache\n",
    "from lightrag.utils import detect_language\n",
    "with open(\"C:\\\\Users\\\\mhieu\\\\Desktop\\\\TN\\\\LIGHTRAG\\\\api_keys.json\", 'r', encoding='utf-8') as f:\n",
    "    OPENROUTER_API_KEYS = json.load(f)\n",
    "\n",
    "class APIManager:\n",
    "    def __init__(self, api_keys: List[str]):\n",
    "        self.api_keys = api_keys\n",
    "        self.current_key_index = 0\n",
    "        self.failed_keys = set()\n",
    "        self.last_switch_time = {}  \n",
    "        \n",
    "    def get_current_api_key(self):\n",
    "        return self.api_keys[self.current_key_index]\n",
    "    \n",
    "    def switch_to_next_key(self):\n",
    "        \n",
    "        self.failed_keys.add(self.current_key_index)\n",
    "        self.last_switch_time[self.current_key_index] = time.time()\n",
    "        \n",
    "        available_keys = []\n",
    "        for idx in range(len(self.api_keys)):\n",
    "            if idx not in self.failed_keys:\n",
    "                available_keys.append(idx)\n",
    "            elif idx in self.last_switch_time:\n",
    "                # Nếu đã qua 10 phút kể từ lần cuối sử dụng key này\n",
    "                if time.time() - self.last_switch_time[idx] > 600:\n",
    "                    self.failed_keys.remove(idx)\n",
    "                    available_keys.append(idx)\n",
    "        \n",
    "        if not available_keys:\n",
    "            raise RuntimeError(\"Tất cả API keys đều đã thất bại. Vui lòng thử lại sau.\")\n",
    "        \n",
    "        self.current_key_index = random.choice(available_keys)\n",
    "        print(f\"Đã chuyển sang API key: {self.api_keys[self.current_key_index][:5]}...\")\n",
    "        return self.get_current_api_key()\n",
    "    \n",
    "    def reset_key(self, key_index):\n",
    "        if key_index in self.failed_keys:\n",
    "            self.failed_keys.remove(key_index)\n",
    "\n",
    "# Khởi tạo API Manager\n",
    "api_manager = APIManager(OPENROUTER_API_KEYS)\n",
    "\n",
    "WORKING_DIR = \"./test_duo\"\n",
    "\n",
    "if not os.path.exists(WORKING_DIR):\n",
    "    os.mkdir(WORKING_DIR)\n",
    "\n",
    "current_api_key = api_manager.get_current_api_key()\n",
    "    \n",
    "async def llm_model_func(\n",
    "    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs\n",
    ") -> str:\n",
    "    global current_api_key\n",
    "    max_retries = len(OPENROUTER_API_KEYS)\n",
    "    retry_count = 0\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            response = await openai_complete_if_cache(\n",
    "                \"google/gemini-2.0-flash-exp:free\",\n",
    "                prompt,\n",
    "                system_prompt=system_prompt,\n",
    "                history_messages=history_messages,\n",
    "                api_key=current_api_key, \n",
    "                base_url=os.getenv(\"LLM_BINDING_HOST\", \"https://openrouter.ai/api/v1\"),\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            key_index = OPENROUTER_API_KEYS.index(current_api_key)\n",
    "            api_manager.reset_key(key_index)\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi với API key: {str(e)}\")\n",
    "            retry_count += 1\n",
    "            \n",
    "            if retry_count < max_retries:\n",
    "                print(\"Đang chuyển sang API key tiếp theo...\")\n",
    "                current_api_key = api_manager.switch_to_next_key()\n",
    "                os.environ[\"LLM_BINDING_API_KEY\"] = current_api_key\n",
    "            else:\n",
    "                print(\"Tất cả API keys đều đã thất bại.\")\n",
    "                raise e\n",
    "    \n",
    "    raise RuntimeError(\"Tất cả API keys đều đã thất bại\")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "print(\"google/gemini-2.0-flash-exp:free\")\n",
    "\n",
    "os.environ[\"LLM_BINDING_API_KEY\"] = current_api_key\n",
    "\n",
    "rag = LightRAG(\n",
    "    working_dir=WORKING_DIR,\n",
    "    llm_model_func=llm_model_func,\n",
    "    embedding_func=EmbeddingFunc(\n",
    "        embedding_dim=1024,\n",
    "        max_token_size=5000,\n",
    "        func=lambda texts: hf_embed(\n",
    "            texts,\n",
    "            tokenizer=AutoTokenizer.from_pretrained(\n",
    "                \"BAAI/bge-m3\"\n",
    "            ),\n",
    "            embed_model=AutoModel.from_pretrained(\n",
    "                \"BAAI/bge-m3\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    addon_params={\n",
    "        # \"insert_batch_size\": 20,\n",
    "        \"language\": \"Vietnamese\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# def insert_with_retry(data_original, data_translated, target_language=\"English\"):\n",
    "#     global current_api_key\n",
    "#     max_retries = len(OPENROUTER_API_KEYS)\n",
    "#     retry_count = 0\n",
    "    \n",
    "#     while retry_count < max_retries:\n",
    "#         try:\n",
    "  \n",
    "#             os.environ[\"LLM_BINDING_API_KEY\"] = current_api_key\n",
    "#             rag.insert_duo(data_original, data_translated, target_language=\"English\")\n",
    "#             print(\"Chèn dữ liệu thành công!\")\n",
    "#             return\n",
    "#         except Exception as e:\n",
    "#             error_str = str(e).lower()\n",
    "#             if \"api\" in error_str or \"key\" in error_str or \"rate\" in error_str or \"limit\" in error_str:\n",
    "#                 print(f\"Lỗi API khi chèn dữ liệu: {str(e)}\")\n",
    "#                 retry_count += 1\n",
    "                \n",
    "#                 if retry_count < max_retries:\n",
    "#                     print(\"Đang chuyển sang API key tiếp theo...\")\n",
    "#                     current_api_key = api_manager.switch_to_next_key()\n",
    "#                     os.environ[\"LLM_BINDING_API_KEY\"] = current_api_key\n",
    "#                 else:\n",
    "#                     print(\"Tất cả API keys đều đã thất bại khi chèn dữ liệu.\")\n",
    "#                     raise e\n",
    "#             else:\n",
    "#                 raise e\n",
    "\n",
    "\n",
    "def insert_with_retry(data, language):\n",
    "    global current_api_key\n",
    "    max_retries = len(OPENROUTER_API_KEYS)\n",
    "    retry_count = 0\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "  \n",
    "            os.environ[\"LLM_BINDING_API_KEY\"] = current_api_key\n",
    "            rag.insert(data, language=language,matching_method=\"embedding\")\n",
    "            print(\"Chèn dữ liệu thành công!\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            if \"api\" in error_str or \"key\" in error_str or \"rate\" in error_str or \"limit\" in error_str:\n",
    "                print(f\"Lỗi API khi chèn dữ liệu: {str(e)}\")\n",
    "                retry_count += 1\n",
    "                \n",
    "                if retry_count < max_retries:\n",
    "                    print(\"Đang chuyển sang API key tiếp theo...\")\n",
    "                    current_api_key = api_manager.switch_to_next_key()\n",
    "                    os.environ[\"LLM_BINDING_API_KEY\"] = current_api_key\n",
    "                else:\n",
    "                    print(\"Tất cả API keys đều đã thất bại khi chèn dữ liệu.\")\n",
    "                    raise e\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        \n",
    "        entities_vdb = rag.entities_vdb\n",
    "        entity_id = \"ent-3ec1c563b6bb321152a391a90957a38b\"\n",
    "        # Use proper client._client.get method\n",
    "        result = entities_vdb._client.get([entity_id])\n",
    "        for i, item in enumerate(entities_vdb._client._NanoVectorDB__storage[\"data\"]):\n",
    "            if item[\"__id__\"] == entity_id:\n",
    "                index = i\n",
    "                break\n",
    "        if index is not None:\n",
    "            # Lấy vector từ matrix theo index\n",
    "            vector = entities_vdb._client._NanoVectorDB__storage[\"matrix\"][index]\n",
    "            print(f\"Vector for entity {entity_id}: {vector}\")\n",
    "            # return vector\n",
    "        print(result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi xử lý dữ liệu: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
